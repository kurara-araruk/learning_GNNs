{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GIN\n",
    "\\\n",
    "スカラー形式の式\n",
    "$$\n",
    "\\pmb{h}_v^{(l+1)} = \\text{MLP}^{(l+1)} \\left(\\left( 1 + \\epsilon^{(l+1)} \\right) \\pmb{h}_v^{(l)} + \\sum_{u \\in \\mathcal{N}(v) } \\pmb{h}_u^{(l)}  \\right)\n",
    "$$\n",
    "行列形式の式\n",
    "$$\n",
    "\\mathbf{H}^{(l+1)} = \\text{MLP}^{(l+1)} \\left( \\mathbf{A} + \\left( 1 + \\epsilon^{(l+1)} \\right) \\mathbf{I} \\right) \\mathbf{H}^{(l)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考文献 :\\\n",
    "「How Powerful are Graph Neural Networks?」 \\\n",
    "https://arxiv.org/pdf/1810.00826 \\\n",
    "「PyG / torch_geometric.nn / conv.GINConv」 \\\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GINConv.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch import Tensor\n",
    "from torch_geometric.typing import Adj\n",
    "import torch.nn.init as init\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import to_networkx\n",
    "from torch_geometric import datasets\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from typing import Any, List, Optional, Tuple\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoche,\n",
    "          data,\n",
    "          optimizer,\n",
    "          criterion,\n",
    "          device,\n",
    "          net,\n",
    "          val = False\n",
    "          ):\n",
    "\n",
    "    optimizer.state_dict()['state'] = {}\n",
    "\n",
    "    data = data.to(device)\n",
    "    for i in range(epoche):\n",
    "        net.train() # 訓練フェーズ\n",
    "        optimizer.zero_grad()\n",
    "        out = net(data)\n",
    "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "\n",
    "        pred = out[data.train_mask].argmax(dim=1)\n",
    "        correct = (pred == data.y[data.train_mask]).sum().item()\n",
    "        total = data.train_mask.sum().item()\n",
    "        train_acc = correct / total\n",
    "\n",
    "        if val:\n",
    "            val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "            val_pred = out[data.val_mask].argmax(dim=1)\n",
    "            val_correct = (val_pred == data.y[data.val_mask]).sum().item()\n",
    "            val_total = data.val_mask.sum().item()\n",
    "            val_acc = val_correct / val_total\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if val:\n",
    "            print(f'Epoch: {i+1} \\n Train loss: {loss:.5f}, Train acc:{train_acc:.5f}, Val loss: {val_loss:.5f}, Val acc:{val_acc:.5f}')\n",
    "        else:\n",
    "            print(f'Epoch: {i+1} \\n Train loss: {loss}, Train acc:{train_acc:.5f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# デバイスの確認\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch ベース"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GINlayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_d: int,\n",
    "        hidden_d: list,\n",
    "        train_eps: bool = True,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        num_MLP_layer = len(hidden_d)\n",
    "\n",
    "        self.mlp = torch.nn.ModuleList()\n",
    "\n",
    "        # 学習パラメータの設定\n",
    "        if train_eps:\n",
    "            self.eps = nn.Parameter(torch.empty(()))\n",
    "\n",
    "        else:\n",
    "            self.register_buffer('eps', torch.tensor(0.0))\n",
    "\n",
    "        # MLP の設定\n",
    "        for i in range(num_MLP_layer):\n",
    "            if i == 0:\n",
    "                self.mlp.append(nn.Linear(in_d, hidden_d[i]))\n",
    "\n",
    "            else:\n",
    "                self.mlp.append(nn.BatchNorm1d(hidden_d[i - 1]))\n",
    "                self.mlp.append(nn.ReLU())\n",
    "                self.mlp.append(nn.Linear(hidden_d[i-1], hidden_d[i]))\n",
    "\n",
    "        if train_eps:\n",
    "            # パラメータの初期化\n",
    "            self.reset_parameters()\n",
    "\n",
    "    # 初期化関数\n",
    "    def reset_parameters(self):\n",
    "        init.normal_(self.eps)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        edge_index: Adj\n",
    "        ):\n",
    "\n",
    "        num_nodes = x.shape[0]\n",
    "        # 隣接行列を作る\n",
    "        A = torch.zeros((num_nodes, num_nodes)).to(device)\n",
    "        A[edge_index[0], edge_index[1]] = 1\n",
    "        # 単位行列の作成\n",
    "        I = torch.eye(num_nodes).to(device)\n",
    "\n",
    "        eps_plus_one = 1 + self.eps\n",
    "        new_A = A + (eps_plus_one * I)\n",
    "\n",
    "        new_H = torch.matmul(new_A, x)\n",
    "\n",
    "        for layer in self.mlp:\n",
    "            new_H = layer(new_H)\n",
    "\n",
    "        return new_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# グラフ畳み込みネットワークの定義\n",
    "class GIN(torch.nn.Module):\n",
    "    def __init__(self, in_d_1, hidden_d_1, in_d_2, hidden_d_2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = GINlayer(in_d_1, hidden_d_1, train_eps=False)\n",
    "        self.conv2 = GINlayer(in_d_2, hidden_d_2)\n",
    "\n",
    "        self.bn = nn.BatchNorm1d(in_d_2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GIN(\n",
      "  (conv1): GINlayer(\n",
      "    (mlp): ModuleList(\n",
      "      (0): Linear(in_features=1433, out_features=1433, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (conv2): GINlayer(\n",
      "    (mlp): ModuleList(\n",
      "      (0): Linear(in_features=1433, out_features=1024, bias=True)\n",
      "      (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=1024, out_features=7, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (bn): BatchNorm1d(1433, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.Planetoid(root='data/Planetoid', name='Cora')\n",
    "data = dataset[0]\n",
    "\n",
    "in_d_1 = dataset.num_node_features\n",
    "hidden_d_1 = [in_d_1]\n",
    "\n",
    "in_d_2 = hidden_d_1[-1]\n",
    "hidden_d_2 = [1024, dataset.num_classes]\n",
    "\n",
    "net = GIN(in_d_1, hidden_d_1, in_d_2, hidden_d_2).to(device)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \n",
      " Train loss: 1.98614, Train acc:0.12857, Val loss: 1.92502, Val acc:0.28600\n",
      "Epoch: 2 \n",
      " Train loss: 1.58627, Train acc:0.37143, Val loss: 1.66839, Val acc:0.44200\n",
      "Epoch: 3 \n",
      " Train loss: 1.46488, Train acc:0.55714, Val loss: 1.61244, Val acc:0.48000\n",
      "Epoch: 4 \n",
      " Train loss: 1.37898, Train acc:0.68571, Val loss: 1.57231, Val acc:0.49800\n",
      "Epoch: 5 \n",
      " Train loss: 1.30807, Train acc:0.69286, Val loss: 1.53852, Val acc:0.51400\n",
      "Epoch: 6 \n",
      " Train loss: 1.24698, Train acc:0.72143, Val loss: 1.50886, Val acc:0.53200\n",
      "Epoch: 7 \n",
      " Train loss: 1.19317, Train acc:0.74286, Val loss: 1.48202, Val acc:0.53800\n",
      "Epoch: 8 \n",
      " Train loss: 1.14506, Train acc:0.75714, Val loss: 1.45801, Val acc:0.55400\n",
      "Epoch: 9 \n",
      " Train loss: 1.10154, Train acc:0.79286, Val loss: 1.43589, Val acc:0.56400\n",
      "Epoch: 10 \n",
      " Train loss: 1.06193, Train acc:0.80714, Val loss: 1.41520, Val acc:0.57800\n",
      "Epoch: 11 \n",
      " Train loss: 1.02558, Train acc:0.80714, Val loss: 1.39625, Val acc:0.58600\n",
      "Epoch: 12 \n",
      " Train loss: 0.99197, Train acc:0.83571, Val loss: 1.37829, Val acc:0.59800\n",
      "Epoch: 13 \n",
      " Train loss: 0.96077, Train acc:0.83571, Val loss: 1.36172, Val acc:0.60600\n",
      "Epoch: 14 \n",
      " Train loss: 0.93170, Train acc:0.84286, Val loss: 1.34598, Val acc:0.61000\n",
      "Epoch: 15 \n",
      " Train loss: 0.90442, Train acc:0.85000, Val loss: 1.33124, Val acc:0.61000\n",
      "Epoch: 16 \n",
      " Train loss: 0.87881, Train acc:0.86429, Val loss: 1.31718, Val acc:0.61600\n",
      "Epoch: 17 \n",
      " Train loss: 0.85480, Train acc:0.87857, Val loss: 1.30429, Val acc:0.62400\n",
      "Epoch: 18 \n",
      " Train loss: 0.83215, Train acc:0.89286, Val loss: 1.29180, Val acc:0.63400\n",
      "Epoch: 19 \n",
      " Train loss: 0.81063, Train acc:0.90000, Val loss: 1.28029, Val acc:0.63600\n",
      "Epoch: 20 \n",
      " Train loss: 0.79015, Train acc:0.91429, Val loss: 1.26900, Val acc:0.64200\n",
      "Epoch: 21 \n",
      " Train loss: 0.77071, Train acc:0.92143, Val loss: 1.25851, Val acc:0.64400\n",
      "Epoch: 22 \n",
      " Train loss: 0.75220, Train acc:0.92143, Val loss: 1.24830, Val acc:0.64600\n",
      "Epoch: 23 \n",
      " Train loss: 0.73455, Train acc:0.92143, Val loss: 1.23873, Val acc:0.64600\n",
      "Epoch: 24 \n",
      " Train loss: 0.71762, Train acc:0.92143, Val loss: 1.22926, Val acc:0.65600\n",
      "Epoch: 25 \n",
      " Train loss: 0.70137, Train acc:0.92143, Val loss: 1.22044, Val acc:0.66200\n",
      "Epoch: 26 \n",
      " Train loss: 0.68580, Train acc:0.92143, Val loss: 1.21183, Val acc:0.66200\n",
      "Epoch: 27 \n",
      " Train loss: 0.67088, Train acc:0.92143, Val loss: 1.20357, Val acc:0.67200\n",
      "Epoch: 28 \n",
      " Train loss: 0.65653, Train acc:0.92143, Val loss: 1.19556, Val acc:0.67400\n",
      "Epoch: 29 \n",
      " Train loss: 0.64279, Train acc:0.93571, Val loss: 1.18809, Val acc:0.67600\n",
      "Epoch: 30 \n",
      " Train loss: 0.62959, Train acc:0.93571, Val loss: 1.18059, Val acc:0.68000\n",
      "Epoch: 31 \n",
      " Train loss: 0.61685, Train acc:0.93571, Val loss: 1.17378, Val acc:0.67800\n",
      "Epoch: 32 \n",
      " Train loss: 0.60463, Train acc:0.94286, Val loss: 1.16673, Val acc:0.67600\n",
      "Epoch: 33 \n",
      " Train loss: 0.59284, Train acc:0.94286, Val loss: 1.16031, Val acc:0.68000\n",
      "Epoch: 34 \n",
      " Train loss: 0.58145, Train acc:0.95000, Val loss: 1.15385, Val acc:0.68400\n",
      "Epoch: 35 \n",
      " Train loss: 0.57041, Train acc:0.96429, Val loss: 1.14783, Val acc:0.68600\n",
      "Epoch: 36 \n",
      " Train loss: 0.55973, Train acc:0.96429, Val loss: 1.14169, Val acc:0.69000\n",
      "Epoch: 37 \n",
      " Train loss: 0.54939, Train acc:0.96429, Val loss: 1.13603, Val acc:0.69200\n",
      "Epoch: 38 \n",
      " Train loss: 0.53937, Train acc:0.97143, Val loss: 1.13044, Val acc:0.69600\n",
      "Epoch: 39 \n",
      " Train loss: 0.52959, Train acc:0.97143, Val loss: 1.12494, Val acc:0.70000\n",
      "Epoch: 40 \n",
      " Train loss: 0.52009, Train acc:0.97143, Val loss: 1.11970, Val acc:0.70400\n",
      "Epoch: 41 \n",
      " Train loss: 0.51084, Train acc:0.97143, Val loss: 1.11444, Val acc:0.70600\n",
      "Epoch: 42 \n",
      " Train loss: 0.50190, Train acc:0.97143, Val loss: 1.10955, Val acc:0.71000\n",
      "Epoch: 43 \n",
      " Train loss: 0.49320, Train acc:0.97143, Val loss: 1.10471, Val acc:0.71000\n",
      "Epoch: 44 \n",
      " Train loss: 0.48476, Train acc:0.97143, Val loss: 1.10003, Val acc:0.71000\n",
      "Epoch: 45 \n",
      " Train loss: 0.47653, Train acc:0.97143, Val loss: 1.09552, Val acc:0.71200\n",
      "Epoch: 46 \n",
      " Train loss: 0.46853, Train acc:0.97143, Val loss: 1.09113, Val acc:0.71400\n",
      "Epoch: 47 \n",
      " Train loss: 0.46075, Train acc:0.97143, Val loss: 1.08681, Val acc:0.71600\n",
      "Epoch: 48 \n",
      " Train loss: 0.45316, Train acc:0.97857, Val loss: 1.08270, Val acc:0.72200\n",
      "Epoch: 49 \n",
      " Train loss: 0.44576, Train acc:0.97857, Val loss: 1.07863, Val acc:0.72200\n",
      "Epoch: 50 \n",
      " Train loss: 0.43854, Train acc:0.97857, Val loss: 1.07478, Val acc:0.72200\n",
      "Epoch: 51 \n",
      " Train loss: 0.43149, Train acc:0.97857, Val loss: 1.07090, Val acc:0.72400\n",
      "Epoch: 52 \n",
      " Train loss: 0.42461, Train acc:0.97857, Val loss: 1.06733, Val acc:0.72400\n",
      "Epoch: 53 \n",
      " Train loss: 0.41788, Train acc:0.97857, Val loss: 1.06348, Val acc:0.72600\n",
      "Epoch: 54 \n",
      " Train loss: 0.41132, Train acc:0.97857, Val loss: 1.05997, Val acc:0.72800\n",
      "Epoch: 55 \n",
      " Train loss: 0.40490, Train acc:0.97857, Val loss: 1.05655, Val acc:0.73200\n",
      "Epoch: 56 \n",
      " Train loss: 0.39863, Train acc:0.97857, Val loss: 1.05320, Val acc:0.73200\n",
      "Epoch: 57 \n",
      " Train loss: 0.39250, Train acc:0.97857, Val loss: 1.04988, Val acc:0.73200\n",
      "Epoch: 58 \n",
      " Train loss: 0.38650, Train acc:0.97857, Val loss: 1.04682, Val acc:0.73400\n",
      "Epoch: 59 \n",
      " Train loss: 0.38063, Train acc:0.97857, Val loss: 1.04359, Val acc:0.73800\n",
      "Epoch: 60 \n",
      " Train loss: 0.37488, Train acc:0.98571, Val loss: 1.04056, Val acc:0.73800\n",
      "Epoch: 61 \n",
      " Train loss: 0.36925, Train acc:0.98571, Val loss: 1.03750, Val acc:0.74000\n",
      "Epoch: 62 \n",
      " Train loss: 0.36377, Train acc:0.98571, Val loss: 1.03480, Val acc:0.74000\n",
      "Epoch: 63 \n",
      " Train loss: 0.35839, Train acc:0.98571, Val loss: 1.03158, Val acc:0.74000\n",
      "Epoch: 64 \n",
      " Train loss: 0.35313, Train acc:0.98571, Val loss: 1.02901, Val acc:0.74000\n",
      "Epoch: 65 \n",
      " Train loss: 0.34799, Train acc:0.98571, Val loss: 1.02619, Val acc:0.73800\n",
      "Epoch: 66 \n",
      " Train loss: 0.34296, Train acc:0.98571, Val loss: 1.02370, Val acc:0.74000\n",
      "Epoch: 67 \n",
      " Train loss: 0.33803, Train acc:0.98571, Val loss: 1.02096, Val acc:0.74200\n",
      "Epoch: 68 \n",
      " Train loss: 0.33321, Train acc:0.99286, Val loss: 1.01854, Val acc:0.74200\n",
      "Epoch: 69 \n",
      " Train loss: 0.32848, Train acc:0.99286, Val loss: 1.01609, Val acc:0.74000\n",
      "Epoch: 70 \n",
      " Train loss: 0.32382, Train acc:0.99286, Val loss: 1.01359, Val acc:0.74000\n",
      "Epoch: 71 \n",
      " Train loss: 0.31928, Train acc:0.99286, Val loss: 1.01128, Val acc:0.74200\n",
      "Epoch: 72 \n",
      " Train loss: 0.31481, Train acc:0.99286, Val loss: 1.00898, Val acc:0.74400\n",
      "Epoch: 73 \n",
      " Train loss: 0.31043, Train acc:0.99286, Val loss: 1.00655, Val acc:0.74600\n",
      "Epoch: 74 \n",
      " Train loss: 0.30613, Train acc:0.99286, Val loss: 1.00456, Val acc:0.74600\n",
      "Epoch: 75 \n",
      " Train loss: 0.30190, Train acc:0.99286, Val loss: 1.00221, Val acc:0.74600\n",
      "Epoch: 76 \n",
      " Train loss: 0.29776, Train acc:0.99286, Val loss: 1.00005, Val acc:0.74600\n",
      "Epoch: 77 \n",
      " Train loss: 0.29370, Train acc:0.99286, Val loss: 0.99796, Val acc:0.74600\n",
      "Epoch: 78 \n",
      " Train loss: 0.28971, Train acc:0.99286, Val loss: 0.99588, Val acc:0.74600\n",
      "Epoch: 79 \n",
      " Train loss: 0.28578, Train acc:0.99286, Val loss: 0.99370, Val acc:0.74600\n",
      "Epoch: 80 \n",
      " Train loss: 0.28195, Train acc:0.99286, Val loss: 0.99176, Val acc:0.74400\n",
      "Epoch: 81 \n",
      " Train loss: 0.27819, Train acc:0.99286, Val loss: 0.98983, Val acc:0.74400\n",
      "Epoch: 82 \n",
      " Train loss: 0.27451, Train acc:0.99286, Val loss: 0.98793, Val acc:0.74400\n",
      "Epoch: 83 \n",
      " Train loss: 0.27089, Train acc:0.99286, Val loss: 0.98610, Val acc:0.74400\n",
      "Epoch: 84 \n",
      " Train loss: 0.26734, Train acc:0.99286, Val loss: 0.98430, Val acc:0.74800\n",
      "Epoch: 85 \n",
      " Train loss: 0.26384, Train acc:0.99286, Val loss: 0.98248, Val acc:0.74800\n",
      "Epoch: 86 \n",
      " Train loss: 0.26042, Train acc:0.99286, Val loss: 0.98090, Val acc:0.74800\n",
      "Epoch: 87 \n",
      " Train loss: 0.25703, Train acc:0.99286, Val loss: 0.97908, Val acc:0.75200\n",
      "Epoch: 88 \n",
      " Train loss: 0.25371, Train acc:0.99286, Val loss: 0.97756, Val acc:0.75200\n",
      "Epoch: 89 \n",
      " Train loss: 0.25044, Train acc:0.99286, Val loss: 0.97568, Val acc:0.75200\n",
      "Epoch: 90 \n",
      " Train loss: 0.24723, Train acc:1.00000, Val loss: 0.97418, Val acc:0.75200\n",
      "Epoch: 91 \n",
      " Train loss: 0.24408, Train acc:1.00000, Val loss: 0.97248, Val acc:0.75400\n",
      "Epoch: 92 \n",
      " Train loss: 0.24099, Train acc:1.00000, Val loss: 0.97108, Val acc:0.75200\n",
      "Epoch: 93 \n",
      " Train loss: 0.23796, Train acc:1.00000, Val loss: 0.96954, Val acc:0.75400\n",
      "Epoch: 94 \n",
      " Train loss: 0.23499, Train acc:1.00000, Val loss: 0.96812, Val acc:0.75400\n",
      "Epoch: 95 \n",
      " Train loss: 0.23206, Train acc:1.00000, Val loss: 0.96652, Val acc:0.75400\n",
      "Epoch: 96 \n",
      " Train loss: 0.22919, Train acc:1.00000, Val loss: 0.96533, Val acc:0.75200\n",
      "Epoch: 97 \n",
      " Train loss: 0.22635, Train acc:1.00000, Val loss: 0.96389, Val acc:0.75200\n",
      "Epoch: 98 \n",
      " Train loss: 0.22357, Train acc:1.00000, Val loss: 0.96256, Val acc:0.75200\n",
      "Epoch: 99 \n",
      " Train loss: 0.22083, Train acc:1.00000, Val loss: 0.96125, Val acc:0.75200\n",
      "Epoch: 100 \n",
      " Train loss: 0.21813, Train acc:1.00000, Val loss: 0.95994, Val acc:0.75200\n",
      "CPU times: user 10min 21s, sys: 1.13 s, total: 10min 22s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr=0.01\n",
    "optimizer = optim.SGD(net.parameters(), lr=lr)\n",
    "\n",
    "%time train(100, data=data, optimizer=optimizer, criterion =criterion, net=net, device=device, val=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
