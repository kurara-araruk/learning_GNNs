{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAT\n",
    "\\\n",
    "スカラー形式の式\n",
    "$$\n",
    "\\pmb{h}_v^{(l+1)} = \\sigma \\left( \\sum_{u \\in \\mathcal{N}(v)}{\\alpha_{vu}^{(l+1)} \\mathbf{W}^{(l+1)} \\pmb{h}_u^{l} } \\right) \\\\\n",
    "$$\n",
    "$$\n",
    "\\alpha_{vu}^{(l+1)} = \\frac{\\exp \\left( \\text{LeakyReLU}  \\left( \\pmb{a}^{(l+1)T} \\lbrack \\mathbf{W}_a^{(l+1)} \\pmb{h}_v^{l} \\Vert \\mathbf{W}_a^{(l+1)} \\pmb{h}_u^{l} \\rbrack \\right) \\right) } { \\sum\\limits_{w \\in \\mathcal{N}(v)} \\exp \\left( \\text{LeakyReLU}  \\left( \\pmb{a}^{(l+1)T} \\lbrack \\mathbf{W}_a^{(l+1)} \\pmb{h}_v^{l} \\Vert \\mathbf{W}_a^{(l+1)} \\pmb{h}_w^{l} \\rbrack \\right) \\right) }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考文献 :\\\n",
    "「Graph Attention Networks」 \\\n",
    "https://arxiv.org/abs/1710.10903 \\\n",
    "佐藤竜馬: 「機械学習プロフェッショナルシリーズ グラフニューラルネットワーク」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch ベース"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "佐藤竜馬: 「機械学習プロフェッショナルシリーズ グラフニューラルネットワーク」 式実装 \\\n",
    "現論文ではWとW_aは共通の重みを使用するが、今回は別とする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATlayer_a(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        att_in_channels: int,\n",
    "        att_out_channels: int,\n",
    "        leaky_relu_slope: float = 0.2\n",
    "        ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.att_in_channels = att_in_channels\n",
    "        self.att_out_channels = att_out_channels\n",
    "\n",
    "        # 学習パラメータの設定\n",
    "        self.a = Parameter(torch.empty(self.att_out_channels * 2, 1))\n",
    "        self.W = Linear(self.in_channels, self.out_channels)\n",
    "        self.W_a = Linear(self.att_in_channels, self.att_out_channels)\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(leaky_relu_slope)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.W.reset_parameters()\n",
    "        self.W_a.reset_parameters()\n",
    "        init.xavier_uniform_(self.a)\n",
    "\n",
    "\n",
    "    def _get_attention_scores(self,\n",
    "                              h: Tensor\n",
    "                              ):\n",
    "\n",
    "        h_tilde = self.W_a(h)\n",
    "\n",
    "        a_weighted_h_v = torch.matmul(h_tilde, self.a[:self.att_out_channels, :])\n",
    "        a_weighted_h_u = torch.matmul(h_tilde, self.a[self.att_out_channels:, :])\n",
    "\n",
    "        e = a_weighted_h_v + a_weighted_h_u.mT\n",
    "\n",
    "        return self.leakyrelu(e)\n",
    "\n",
    "    def forward(self,\n",
    "                h: Tensor,\n",
    "                edge_index: Adj\n",
    "                ):\n",
    "\n",
    "        # 隣接行列を作る\n",
    "        num_nodes = h.shape[0]\n",
    "        A = torch.zeros((num_nodes, num_nodes)).to(device)\n",
    "        A[edge_index[0], edge_index[1]] = 1\n",
    "        # 自己ループ持ち隣接行列を作る\n",
    "        I = torch.eye(num_nodes).to(device)\n",
    "        A = A + I\n",
    "\n",
    "        HW = self.W(h)\n",
    "\n",
    "        e = self._get_attention_scores(h)\n",
    "        self.e = e.detach()\n",
    "\n",
    "        connectivity_mask = -9e16 * torch.ones_like(e)\n",
    "        e = torch.where(A > 0, e, connectivity_mask)\n",
    "\n",
    "        attention = F.softmax(e, dim=1)\n",
    "        self.attention = attention\n",
    "\n",
    "        H_new = torch.matmul(attention, HW)\n",
    "\n",
    "        return H_new"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
